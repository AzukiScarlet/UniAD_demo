# UniAD 代码阅读

## 代码结构

### 代码目录

```shell
UniAD
├── tools
    ├──train.py
    ├──test.py
    ├──analysis
    ├──dataconvert
├── projects
    ├──configs：模型网络配置文件
    ├──mmdet3d: 3D目标检测相关
        ├──**uniad：UniAD项目相关(models)**
            ├──modules：模型网络结构
            ├──api：模型网络接口
            ├──...
        ├──其他
├──ckpts：模型训练权重
├──data：数据集

```

### 训练框架

- `tools/train.py`：训练脚本
- `tools/test.py`：测试脚本

在 `tools/uniad_dist_train.sh`中调用 `tools/train.py`进行训练，`tools/uniad_dist_eval.sh`中调用 `tools/test.py`进行测试。内部进行了输入输出，GPU等配置。

## 模型网络

`projects/configs`下有两阶段的模型网络配置文件，分别是 `stage1_track_map/base_track_map.py`和 `stage2_e2e/base_e2e.py`。

其中 `model = dict(type='UniAD', ...)`指定了模型网络的各个模块。

每个模块的具体实现在 `projects/mmdet3d_plugin`中，自己搜 `type`的值。

**UniAD本身的模型架构在 `projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py`可以看到**

下面介绍各个模块结构和输出

## Train

以下各个模块均为顺序执行

- ?: 在第一阶段和第二阶段的配置中，都是 `as_two_stage=false`，mapformer和trackformer的decoder都是用可学习的Q，不用encoder的最后一层的Q。

### Track Former

BEV也在其中实现

- 输入：一堆图片
- 输出：`losses_track`, `outs_track`: 包含**BEV相关**(嵌入，位置编码)；track相关(**Q_a**，匹配的索引，bbox结果)；自车相关(3d相关，track相关，**Q_sdv**)

#### 结构

1. 初始化一个空的track实例(**关键的是Q**)
2. 遍历历史帧图像[B, num_cam, 3, H, W]

   1. 做 `BEVFormerTrackHead`的**encoder**，生成BEV embedding 和位置编码，本质是一个encoder，输出就是最后一层的Q。encoder的：

      - Q: 初始化一个可学习的Q
      - K,V： img过一个embedding再处理一下(这里融合了canbus信息)

   2. 做 `BEVFormerTrackHead`的**decoder**输出(分类结果， 预测边界， 最后的参考点， **Q**)decoder的：

      - Q：步骤一初始化的最外层track实例的Q
      - K,V：encoder的输出 BEV embedding

   3. 处理每个decoder层的输出
   4. 使用匹配器更新track实例
3. 滚动更新num_layers次后输出 `outs_track`(从track实例中提取)和 `losses_track`

### Map Former

- 输入：BEV嵌入
- 输出：`losses_seg`, `outs_seg`: `outs_seg`是一个字典，包含了 `(outputs_classes, outputs_coords, enc_outputs_class, enc_outputs_coord, args_tuple, reference)`其中args_tuple是一个元组，包含了(memory, memory_mask, memory_pos, **query**, _, query_pos, hw_lvl)
  - memory相关：endcoder的每一层的输出
  - query相关：decoder的最后一层的输出 **Q_m**
  - hw_lvl：BEV的展平后的特征尺寸

#### 结构

1. 转换BEV为[N, C, H, W]，创建mask和位置编码
2. 初始化decoder中使用的Q(query_embeds):
   1. `as_two_stage=False`使用可学习的Q
   > `as_two_stage=True`时使用encoder的最后一层的Q，但是**第一阶段和第二阶段都不用都**
3. 做 `SegDeformableTransformer`。
   1. encoder：Q, K, V = BE
   2. decoder：Q：使用可学习的Q， K： 不需要，Attn直接由Q得到（K = Q）， V：encoder的输出feature map
4. 对输出处理，遍历每个解码层，计算分类和回归输出
5. 计算loss

### 第二阶段的变化

- 引入`MotionFormer`,`OccFormer`，`Planner`构成了`UniAD`全面架构
- **冻结了BEV相关网络参数**:
    ```python
    freeze_img_backbone=True,
    freeze_img_neck=True,      #* 第二阶段冻结图像颈部网络
    freeze_bn=True,            #* 冻结BN层
    freeze_bev_encoder=True,   #****** 冻结BEV编码器
    ```
- 在第二阶段train中仍然，`as_two_stage=False`，decoder不使用encoder的最后一层的Q，而是使用可学习的Q。
